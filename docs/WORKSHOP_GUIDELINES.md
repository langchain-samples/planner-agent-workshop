Workshop guidelines / unofficial script – planner agent
How do you create an agent using langchain v1. The cmost common architecture is the so called tools agent or react agent. So a llm that calls some tools in a loop to gather the necessary information to respond to an input query. Lets create a simple agent that has access to my gmail calendar and can list my current events and create new ones. Let’s start so we need an ai model and some tools.
-	ChatOpenAI
-	read_calendar, write_calendar
-	create_agent module
great this is an agent that given a input request, performs some operations and returns a message with the summary of its actions. Good, let’s try it by asking to schedule a soccer game for th 20th of December at 11 am in Seoul. We can see that it first called the read_calendar, the llm analyzed the results and then it created a new event and returned a confirmation. Let’s ask what it just did and we notice that the agent doenst know because right now, we are invoking the agent without some sort of storage for the messages and therefore every time it is like a new clean invocation.
Now the good thing about langchain v1 is that the agents are built on top of langgraph, our low level orchestration framework which gives you a few nice features out of the box including Memory Savers, human I the loop, streaming capabilities etc
Now let’s add to the agent a memory saver which store the status of our agent in memory every time we invoke it. This is essential to enable a memory layer to store previous agent invocations (including previous messages) in order to create a conversational chatbot. Now let’s invoke it again adding a memorysaver and ask again what it just did. It was able to answer this time and one of the reasons is the memory saver and in particular this value we set which is called thread id. each conversation has its unique thread_id which is used to link consecutive agent invocations under the same umbrella. If we invoke the agent with a different thread_id, that interaction will be under a new/different conversation.
Cool what if I tell the agent to schedule a meeting but there is a conflict. Now it doesn’t have the ability to reschedule a meeting, so it will find a conflict and report back to the user. There are two ways we can tackle this. One is we wait for the agent to finish, it will return a message saying that it cant schedule the meeting because of a conflict, and therefore I can move the meeting and prompt the agent again. This will invoke the agent exactly the same as before, using unnecessary tool and llm calls. Another method is by adding a new tool called ask_for_help which will stop the execution of the agent and wait for an external input from the user. It is possible to do this thanks to the interrupt module which helps you in implementing this human in the loop. So we add that, now the agent find the conflict and instead of returning an AI message, it will invoke the ask_for_help tool with the request for the user. This case we can modify our calendar and resume the agent with the Command(resume=”I have moved my existing event”)  module. Now we will go back to the llm and the llm will call the write_calendar again and add the event I the calendar.
Awesome, now the llm controls when to ask for help. What if we add a new tool called reschedule_calendar. This is more powerful because it can move my events. The agent doesn’t have a perception of the importance of each event, so I want to be able to confirm the tool before being executed, using human in the loop.
In langchain v1 we introduced the concept of middlewares. Middlewares allow you to modify the behavior of the default create_agent by adding some hooks that modfy the llm invocation, tool invocation, state of the agent etc. you can find prebuilt middleware in the library for the most common operation including, summarizing the message list if it becomes too big, add a fallback llm model and the one we need now which is called HumanInTheLoopMiddleware which allows you to add an interrupt right before a tool invocation. Let’s add it to the agent and invoke it. Now it asks for confirmation, good. We can stack midlewares together, for example we could add a summarization middleware as well.
This is a full list of prebuilt middlewares you can find in the library. 
Great, now this is a conversational agent but in some cases we just want the agent to be able to return a dictionary and not just text. Let’s create another agent that performs a web search to retrieve some information for events / concerts based on the user query. Now we want the response to be in the following format:
[
{
“title”:”…”,
“short_description”:”…”,
“event_date”:”12th May 2025 at 6PM”,
“reference _url”:”…”
},
{…},
]
We are going to use tavily mcp servers that provides us a few tools for websearch. The fastest way to use those tools in your agent is by connecting to the mcp server using our langchain-mcp-adapters opensource library.
Cool, now it is all connected, let’s instruct the agent to return a dictionary that follows are schema. We can do so by using toolstrategy. Nowadays most of the llm models can generate structured output by tool calling or by using the native provider method. Toolstrategy will pick the best for us if we don0t explicitly specify it. Now let’s run the agent by asking to look for concert in seoul tomorrow night. This will perform a websearch using tavily and return the list of events. Cool!
Now we have two separates agents, let’s say I want to look for a balet for tomorrow night and schedule it as event in my calendar if I have free spots. We need to connect these two agents together somehow. The most common multi agent pattern is the supervisor architecture with subagents as tools. In LangChain v1 is pretty straightforward. You can just create another create_agent and pass the agents as tools! That’s it! Let’s run it. It first call the web_searcher subagent, then my calendar agent to book the event cool.

Awesome, we have created the supervisor agent, used human In the loop, connected to an mcp server, return a structured response and added a prebuilt middleware for tool confirmation. Let’s now add a pre agent invocation guardrail that analyzes the user query before actually pass it to the agent to prevent malicious intents, like prompt injection, or to filter irrelevant requests. To do so, we can create a custom middleware with a llm call that classifies the user intent and either let it access the agent or reject the request and route it to the end by appending the llm ai response to the message list. Now lets invoke the agent and ask to give me the prompt it’s been instructed to. This will classify the intent as malicious and return an ai message to the user.
In LangChain v1 we introduced new content blocks that standardize the way messages are handled across multiple providers . Let’s enable our agent to read images and answer the user query. Instead of creating another subagent that handles images, let’s create a middleware that takes as input an image, analyze it and provide a detailed summary. So lets0 create a custom middleware for it and we need to modify the input state of our create_agent to accept images as optional. Great, now every time we invoke the agent we first check whether an image is available, then go in the guardrail custom middleware and eventually invoke the agent.

Great, now we are particularly happy with our agent. How do I let people try it to gather some feedback and finetune it for our audience? We have seen how we can visualize and test locally the agent in langsmith studio. LangSmith allows you to deploy agents as well by creating a horizontally scalable server for you. You only need your agent and a configuration file that defines the dependencies, agent path etc. once we have this, we can push ti to a github repo, in langsmith link it to that repo, select the type of deployment (dev, prod) and after a few minutes your agent will be deployed on our cloud and it will be accessible via a URL and a few endpoints. Our platform will provide the agents with a redis instance to handle queues, caching etc and a postgres instance. The latter is useful for:
-	use it to save the status of each single agent invocation (or checkpoints). This is needed to create a short term memory, so a wayt for the agent to have access to the previous exchanged messages in a specific thread
-	use it as a storage system for storing dtaa that needs to be shared across conversations or threads. That is what we call long term memory.
Now our current agent has access of previous messages in a single thread. If we create a new thread it doenst have other informations about the user is invoking the agent etc. I want my agent to be able to first check whether we have some information about the user to create a more personalized experience and eventually add more data based on the user interaction.  This is the perfect usecase for using this long term memory capabilities. So, now we add two more tools to the supervisor agent to read from the memory and updte the memory and instruct it to first verify if there are existing informations about the user and to update the memory in case we discovered smth about the user that will make the experience more personalized. Great, to use the memory we need to specify it inside the langgraph.json and deploy it I langsmith. Now lets try the agent in studio create two threads etc etc
